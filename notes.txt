Kubernetes 

kubectl: use for managing containers in the node
minikube: use for managing VM itself and this applies only on local machine, no use on cloud services

command line to check the installation
minikube status
kubectl cluster-info

comparing Kubernetes to docker-compose
    - kub8s expect all images already build, there is no build option => make sure our image is hosted in dockerhub
    - with docker-compose each entry represents a container we want to create, => make one config file to create a container 
    there is no such thing with Kubernetes, once config file per object we want to create
    - with kub8s we have to manually setup all networking => make one config file to set up networking

##### deploying our simple k8s app #####
on the root folder
mkdir simple
create a file : client-pod.yaml

apiVersion: v1
kind: Pod
metadata:
    name: client-pod 
    labels: 
        component: web 
spec:
    containers:
        -   name: client
            image: stephengrider/multi-client 
            ports:
                - containerPort: 3000

create another file to manage the networking setup: client-node-port.yaml

apiVersion: v1
kind: Service
metadata: 
    name: client-node-port
spec:
    type: NodePort
    ports:
        - ports: 3050
          targetPort: 3000
          nodePort: 31515
    selector:
        component: web

Object types: Pod, Service, ReplicaController, StatefulSet

####################

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-world
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
        - name: nginx
          image: nginx

kubectl apply -f hello-world-deployment.yaml
kubectl get deployments
kubectl expose deployment hello-world-deployment --type=NodePort --port=80
curl http://localhost:<NodePort>

kubernetes is a system to deploy a containerized apps
Nodes are individual vm that runs containers
masters are vm with a set of programms to manage pods
pods: runs one or more closely related containders
ClusterIP: exposes a set of pods to other objects inside the cluster
NodePort: exposes a set of pods to the external world
LoadBalancer: legacy way of getting network traffic into a cluster
Ingress: exposes a set of serivces to the outside world
# volumes with database, keeps data outside of container but inside a pod 
# and save data as backup in case the container crashes inside the pod 
# persistance volumes, keeps data outside the pod
Secrets is a type of object that securely stores a piece info in the cluster
such as database password

### install ingress-nginx ###
github.com/kubernetes/ingres-nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml
minikube addons enable ingress

##### k8s cli #####

kubectl apply -f <filename> # feed config file to kubectl
kubectl get pods # print the status of all running pods
kubectl get services # print the status of all running pods
kubectl describe <object type> <object name> # get details about the object 
kubectl delete -f <config file>
kubectl get pods -o wide # for more details
kubectl delete pods/deployment/services <object name>
kubectl get storageclass
kubectl get pv # stands for persistent volumes
kubectl get pvc
kubectl describe storageclass
kubectl create secret generic/docker-registry/tls <secret_name> --from-literal \
key=value
kubectl create secret generic pgpassword --from-literal PGPASSWORD=test@script123
kubectl get secrets
kubectl run postgres-pod --image=postgres --env="POSTGRES_PASSWORD=test@script123"

# imperative command to update image 
kubectl set image <object type> / <object name> <container name> = <image to use>
kubectl set image deployment/cart-api cart-api=hirodaridevdock/shopping_cart:v1
eval $(minikube docker-env) # this command helps to connect your docker server to 
kubernetes vm machines
why using <eval $(minikube docker-env)>, for debugging reasons like:
docker log <container_id>
docker exec -it <container_id> sh

# same can be done with kubectl 
kubectl get pods
kubectl logs <pods_id>

minikube ip # very import this is the ip address to use not localhost
type/kind: Deployment => maintains a set of identical pods , ensuring that
they have the correct config and that the right number exists
minikube dashboard


iam account id: ${iam_account_id}


#### permission for iam user to use eks ####
set up a group: eksadmin with the following permissions
AmazonEC2FullAccess => AWS managed
AWSCloudFormationFullAccess => AWS managed
EKSAccesssLimited =>   customer inline
I => customer inline

EKSAccesssLimited json:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "eks:*",
            "Resource": "*"
        },
        {
            "Action": [
                "ssm:GetParameter",
                "ssm:GetParameters"
            ],
            "Resource": [
                "arn:aws:ssm:*:${iam_account_id}:parameter/aws/*",
                "arn:aws:ssm:*::parameter/aws/*"
            ],
            "Effect": "Allow"
        },
        {
            "Action": [
                "kms:CreateGrant",
                "kms:DescribeKey"
            ],
            "Resource": "*",
            "Effect": "Allow"
        },
        {
            "Action": [
                "logs:PutRetentionPolicy"
            ],
            "Resource": "*",
            "Effect": "Allow"
        }
    ]
}

AMLimitedAccess json:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:GetInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:GetRole",
                "iam:CreateRole",
                "iam:DeleteRole",
                "iam:AttachRolePolicy",
                "iam:PutRolePolicy",
                "iam:AddRoleToInstanceProfile",
                "iam:ListInstanceProfilesForRole",
                "iam:PassRole",
                "iam:DetachRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:GetRolePolicy",
                "iam:GetOpenIDConnectProvider",
                "iam:CreateOpenIDConnectProvider",
                "iam:DeleteOpenIDConnectProvider",
                "iam:TagOpenIDConnectProvider",
                "iam:ListAttachedRolePolicies",
                "iam:TagRole",
                "iam:GetPolicy",
                "iam:CreatePolicy",
                "iam:DeletePolicy",
                "iam:ListPolicyVersions"
            ],
            "Resource": [
                "arn:aws:iam::${iam_account_id}:instance-profile/eksctl-*",
                "arn:aws:iam::${iam_account_id}:role/eksctl-*",
                "arn:aws:iam::${iam_account_id}:policy/eksctl-*",
                "arn:aws:iam::${iam_account_id}:oidc-provider/*",
                "arn:aws:iam::${iam_account_id}:role/aws-service-role/eks-nodegroup.amazonaws.com/AWSServiceRoleForAmazonEKSNodegroup",
                "arn:aws:iam::${iam_account_id}:role/eksctl-managed-*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:GetRole"
            ],
            "Resource": [
                "arn:aws:iam::${iam_account_id}:role/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": [
                        "eks.amazonaws.com",
                        "eks-nodegroup.amazonaws.com",
                        "eks-fargate.amazonaws.com"
                    ]
                }
            }
        }
    ]
}


## storage with AWS 
kubectl get pvc 
kubectlget pv
### Monitoring with prometheus and grafana ###
L: logstash or fluentd, collects or pulls logs from containers 
E: ElasticSearch is a distributed search and analytic engine, you can store and query data based on apache lucine
k: Kibana allows you to visualize your ElasticSearch data, nice frontend platform
fluentd need to be installed in every single node in the cluster(daemonset), where ElasticSearch 
and kibana can be installed anywhere from the cloud provider solution to a dedicated ec2 server available 
files: fluentd-config.yaml and Elastic-stack.yaml for ElasticSearch and kibana.
namespace: kube-system

Kibana: to have access to kibana check the svc and copy LoadBalancer dns name 
kubectl get svc -n kube-system 
once you have access to the page, create an index: for our case logstash* will match every day's index 
logstash* or collected logs for all containers 
add the timefield name @timestamp

on Kibana menu, click on discovery for elastichsearch engine, you will see logs of every pods.
Kibana dashboard: you can play around, find a search key words and get the result and save the 
search.
Go to visualize to choose the template and visualize, the search result

This section helped only to deploy Kibana, the next section is for prometheus and Grafana tools

##### Prometheus and Grafana ####
fluentd and kibana helps to monitor and analyze logs from each pods while
Prometheus and grafana helps monitor and run a cluster: node healthiness, cpu 
it is more for monitoring the hardware rather the application inside containers like with
kibana

prometheus is a multi purpose interface that can be used for many other services, so 
for a better best suited prometheus for k8s, use chart called kube-prometheus-stack 
files required: crds.yaml [custom Resource Definition], eks-monitoring.yaml
make sure you apply crds.yaml first 
namespace: monitoring

Prometheus is the backbone of the monitoring system, it gathers all the information
but dont have rich front end 

kubectl get svc -n monitoring
monitoring-kube-prometheus-prometheus with port 9090 is the service that gives you 
access to the frontend  

kubectl edit svc monitoring-kube-prometheus-prometheus -n monitoring, set it to 
loadbalancer if you want to have access incase ingress is not yet installed

node_load[1-3] metrics used by prometheus to monitor overloaded nodes 
# grafana
kubectl get svc -n monitoring
monitoring-grafana: this is the service you want to expose if you need to access the frontend 
default credentials:
username: admin 
password: prom-operator

from the menu, you can choose a better dashboard style: 
- USE(utilisation, saturation and Error) Method/ Cluster
- Use Method/ Node 
- persistent volume dashboard 

u can choose any dashboard that fit your need 

##### Alert ####
I need to come back and watch videos to complete this section

##### Requests and limits ####
Requests can be memory or cpu resources, Requests goes under containers 
resources: # always under image as benchmark
    requests:
        memory: 300Mi # lowest value is 4Mi causes OOMkilled
        cpu: 100m
    limits:
        memory: 400Mi
        cpu: 200m

kubectl describe node_name[minikube]: to check capacity and dispatch allocation
allocated resources gives you a summary of all

cpu can be allocated in the following way:
1: 1 full cpu allocation
0.5: half cpu allocation
0.1: is equivalent od 100m millicpu or 100 millicores

if the actual memory usage of the container at run time exceeds the limit, the 
container will be killed not the pod, meaning the container will be restarted 
by the pod 

if the actual cpu usage of the container at the run time exceeds the limit, 
the cpu will be "clamped", meaning the container will continue to run while cpu
will not be allowd to go over and no restart involved.

while allocating limits is optional, resources request is good practice allow 
pods to make good decision

##### Metrics Profiling ####
addons for minikube 

minikube addons list 
minikube addons enable metrics-server
kubectl top pod # gives metrics of pods about memory and cpu
kubectl top node
still prefer kubectl describe no minikube
minikube addons enable dashboard #namespace kube-system
minikube dashboard # to call the frontend 

##### Horizontal Pod Auto-Scaling ####
how to make HPA automatically:
1. step:
kubectl autoscale deployment api-gateway --cpu-percent  400 --min 1 --max 4  
2. step:
kubectl get hpa
3. step:
kubectl get hpa api-gateway -o yaml > hpa-api-gateway.yaml

kubectl describe hpa
make sure to match the correct memory allocation and get the limit right
cpu can be managed by hpa 

##### Liveness and readiness probes ####
bash:
while true; do crul some_url:port/api; done
readines probe make sure k8s does not send traffic to a pod 
unless it is ready 
syntax below goes below containers => image.
readinessProbe:
    httpGet:
        path: /
        port: 8080

Liveness probe will continuessly run for the duration of pod lifetime
with httpGet if for any reason that pod fails then k8s will restart
the container 

##### quality of service ####
Qos is defined when you describe a pod, this the rule:
Qos: guarenteed => when request and limit of both cpu and memory are defined 
Qos: burstable => when request of both cpu and memory are defined without limit 
Qos: BestEffort => when no resource is allocated 

##### configMap and secrets ####
kubectl get cm
configMap servers for mapping environment configs
configMap format:

##### for environment #######
apiVersion: v1
kind: ConfigMap
metadata:
    name: spring-profiles-active-v1
    namespace: default
data:
    SPRING_PROFILES_ACTIVE: "production-microservice"

on the pod side: under environment

containers:
- name: some-container 
  image: some-image 
  env:
  - name: DATABASE_URL 
  envFrom:
  - configMapRef:
    name: spring-profiles-active-v1

##### Mount volume inside a container or image #######
same level as env
volumeMounts:
- name: database-config-volume
  mountPath: /etc/any/directory/config
same levels as container
volumes:
- name: database-config-volume
    configMap:
        name: global-database-config

the configMap.yaml 

apiVersion: v1
kind: ConfigMap
metadata:
    name: global-database-config
    namespace: default
data:
    database.ini:
        SPRING_PROFILES_ACTIVE=production-microservice
        DATABASE_URL=https://mydb.somewhere,com:3306
        DATABASE_PASSWORD=P@ssWord

######## secret section #########

apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
data:
  someAccessKey: MTIzNDU2Nzg5Cg==
  someSecretKey: ZWNobyBTRUNSRVQxMjM0Cg==

kubectl get secret 
kubectl get secret -o yaml

######## Ingress section #########
Ingress controller is a resource which allows us to define routing rules 
having multiple services connected to the same LoadBalancer

with minikube check ingress: minikube addons list 
install ingress: minikube addons enable ingress
namespace: kube-system

kubectl get svc --all-namespacesl
kubectl get ingress 

ingress: networking.k8s.io/v1
kind: Ingress
metadata:
    name: fleetman-ingress 
spec:
    rules:
    - host: "fredbitenyo.click"
      http:
        paths:
        - pathType: Prefix
          path: "/"
          backend:
            service:
              name: webapp
              port:
               number: 80

#### AWS section ####
https://kubernetes.github.io/ingress-nginx/
1. step: apply or download k8s ingress controller manifest for cloud 
namespace: ingress-nginx
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
2. step: download nginx ingress controller for NLB
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/aws/deploy.yaml

#!/bin/bash

# Get the DNS name of the Ingress load balancer
INGRESS_DNS=$(aws elbv2 describe-load-balancers  --query 'LoadBalancers[*].DNSName' --output text)

# Update the Ingress YAML file with the obtained DNS name
cat <<EOF > updated-ingress.yaml
$(sed "s/<your_ingress_dns>/$INGRESS_DNS/g" original-ingress.yaml)
EOF

echo "Ingress YAML file updated with the DNS name: $INGRESS_DNS"

aws route53 change-resource-record-sets \
    --hosted-zone-id <hosted_zone_id> \
    --change-batch '{
        "Changes": [{
            "Action": "CREATE",
            "ResourceRecordSet": {
                "Name": "<record_name>",
                "Type": "A",
                "AliasTarget": {
                    "HostedZoneId": "<nlb_hosted_zone_id>",
                    "DNSName": "<nlb_dns_name>",
                    "EvaluateTargetHealth": false
                }
            }
        }]
    }'

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "ssm:GetParametersByPath",
                "ssm:GetParameters"
            ],
            "Resource": [
                "arn:aws:ssm:us-east-1:181224260758:parameter/codebuild/AWS_ACCESS_KEY_ID",
                "arn:aws:ssm:us-east-1:181224260758:parameter/codebuild/AWS_SECRET_ACCESS_KEY"
            ]
        }
    ]
}

########### code deploy instructions ############
# codedeploy agent
configure amazon ec2 instance to work with aws codedeploy
REGION=$(curl 169.254.169.254/latest/meta-data/placement/availability-zone/ \sed 's/[a-z]$//')
sudo yum updatesudo yum install ruby
sudo yum install wgetcd /home/ec2-user 
wget https://aws-codedeploy-$REGION.s3.amazonaws.com/latest/install 
chmod +x ./install 
usdo ./install auto 

## IAM roles ##
1. IAM role for instance profile 
2. IAM role for service profile

instance role name: CodeDeployInstanceRole
attach these policies: [AmazonEC2RoleforAWSCodeDeploy, AutoScalingNotificationAccessRole]
instance role: CodeDeployServiceRole
attach: [AWSCodeDeployRole: edit trust relationship]

format: appspec.yaml 

version:0.0
os: linux
files:
    - source: /index.html
      destination: /var/www/html 
hooks:
    BeforeInstall:
        - location: scripts:/install_dependencies
          timeout: 300
        - location: scripts/start_server
          timeout: 300
          runas: root
    ApplicationStop:
        - location: scripts/stop_server
          timeout: 300
          runas: root

create codedeploy lab template where:
1. install codedeploy agent through user-data, iam role 
2. edit trust relationship:
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "codedeploy.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}


##### deploy eks using ecr #####
docker images --filter reference=webapp
1. create ecr account 
2. push ur images to ecr 
3. 



apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: eu-north-1

nodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 10
    volumeSize: 80
    ssh:
      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
  - name: ng-2
    instanceType: m5.xlarge
    desiredCapacity: 2
    volumeSize: 100
    ssh:
      publicKeyPath: ~/.ssh/ec2_id_rsa.pub

# for an existing vpc
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-in-existing-vpc
  region: eu-north-1

vpc:
  id: vpc-xxxxx
  cidr: "192.168.0.0/16"
  subnets:
    public:
        my-new-stack-PublickSubnet01:
            id: subnet-xxxx
        my-new-stack-PublickSubnet02:
            id: subnet-xxxx
    private:
      eu-north-1a: { id: subnet-0ff156e0c4a6d300c }
      eu-north-1b: { id: subnet-0549cdab573695c03 }
      eu-north-1c: { id: subnet-0426fb4a607393184 }

nodeGroups:
  - name: ng-1-workers
    labels: { role: workers }
    instanceType: m5.xlarge
    desiredCapacity: 10
    privateNetworking: true
  - name: ng-2-builders
    labels: { role: builders }
    instanceType: m5.2xlarge
    desiredCapacity: 2
    privateNetworking: true
    iam:
      withAddonPolicies:
        imageBuilder: true

